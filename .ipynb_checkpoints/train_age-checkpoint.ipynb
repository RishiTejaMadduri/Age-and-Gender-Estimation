{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import sys\n",
    "sys.path.append(\"/home/rishi/Projects/Matroid/Rishi_Challenge/Data\")\n",
    "import data_loader\n",
    "import argparse\n",
    "from vgg import*\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_dataset='/home/rishi/Projects/Matroid/Rishi_Challenge/Data/adience_data_preprocessing/faces/faces_dataset.h5'\n",
    "vgg_weight='/home/rishi/Projects/Matroid/Rishi_Challenge/Data/vgg_data_preprocessing/pkl_weights'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset_loading\n",
    "parser = argparse.ArgumentParser(description=\"Train Age and Gender Estimation\")\n",
    "parser.add_argument(\"--audience_dataset\", type=str, default=face_dataset)\n",
    "parser.add_argument(\"--vgg_weight\", type=str, default=vgg_weight)\n",
    "parser.add_argument(\"--folder_to_test\", default=1)\n",
    "#Model Hyperparameters\n",
    "parser.add_argument(\"--dropout_keep_prob\", type=float, default=0.5, help=\"Dropout keep probability\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=1e-3, help=\"Weight decay rate for L2 regularization\")\n",
    "# Training Parameters\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=1e-2, help=\"Starter Learning Rate\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=64, help=\"Batch Size\")\n",
    "parser.add_argument(\"--num_epochs\", type=int, default=50, help=\"Number of training epochs\")\n",
    "parser.add_argument(\"--evaluate_every\", type=int, default=2, help=\"Evaluate model on dev set after this many steps (default: 50)\")\n",
    "parser.add_argument(\"--moving_average\", type=bool, default=True, help=\"Enable usage of Exponential Moving Average\")\n",
    "\n",
    "args = parser.parse_args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    #Loading Data\n",
    "    train_data, train_label, test_data, test_label, bgr_mean=data_loader.load_dataset(args.audience_dataset, 1)\n",
    "    bgr_mean = [round(x, 4) for x in bgr_mean]\n",
    "    \n",
    "    acc_list = [0]\n",
    "    loss_train_list = [0]\n",
    "    loss_test_list = [0]\n",
    "\n",
    "    sess = tf.Session()\n",
    "    \n",
    "    bgr_mean=[93.5940, 104.7624, 129.1863]\n",
    "    cnn = vgg_face(bgr_mean, args.vgg_weight, num_classes=8, args.weight_decay)\n",
    "    vgg_known_acc_max = [0.65, 0.51, 0.59, 0.49, 0.59]\n",
    "    \n",
    "    global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "    optimizer = tf.train.MomentumOptimizer(args.learning_rate, 0.9)\n",
    "    lr_decay_fn = lambda lr, global_step : tf.train.exponential_decay(lr, global_step, 100, 0.95, staircase=True)\n",
    "    train_op = tf.contrib.layers.optimize_loss(loss=cnn.loss, global_step=global_step, clip_gradients=4.0, learning_rate=args.learning_rate, optimizer=lambda lr: optimizer, learning_rate_decay_fn=lr_decay_fn)\n",
    "    \n",
    "    timestamp = str(int(time.time()))\n",
    "    out_dir = os.path.join(os.path.expanduser('~'), 'volume', \"runs\", timestamp)\n",
    "    print(\"Writing to {}\\n\".format(out_dir))\n",
    "    checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "    checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "\n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    saver = tf.train.Saver(tf.global_variables(), max_to_keep=5)\n",
    "    \n",
    "    \n",
    "    def train_step(x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        A single training step\n",
    "        \"\"\"\n",
    "        feed_dict = {cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: args.dropout_keep_prob}\n",
    "        _, step, loss, accuracy = sess.run([train_op, global_step, cnn.loss, cnn.accuracy], feed_dict)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        print(\"{}: Step {}, Loss {:g}, Acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "\n",
    "    def test_step(x_batch, y_batch):\n",
    "        \"\"\"\n",
    "        Evaluates model on a dev set\n",
    "        \"\"\"\n",
    "        feed_dict = {cnn.input_x: x_batch, cnn.input_y: y_batch, cnn.dropout_keep_prob: 1.0}\n",
    "        loss, preds = sess.run([cnn.loss, cnn.predictions], feed_dict)\n",
    "        time_str = datetime.datetime.now().isoformat()\n",
    "        return preds, loss\n",
    "    \n",
    "    train_batches = data_loader.batch_iter(list(zip(train_data, train_label)), args.batch_size, args.num_epochs)\n",
    "    \n",
    "    for train_batch in train_batches:\n",
    "        x_batch, y_batch = zip(*train_batch)\n",
    "        train_step(x_batch, y_batch)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        evaluate_every=30\n",
    "        # Testing loop\n",
    "        if current_step % args.evaluate_every == 0:\n",
    "            print(\"\\nEvaluation:\")\n",
    "            i = 0\n",
    "            index = 0\n",
    "            sum_loss = 0\n",
    "            test_batches = data_loader.batch_iter(list(zip(test_data, test_label)), args.batch_size, 1)\n",
    "            y_preds = np.ones(shape=len(test_label), dtype=np.int)\n",
    "            for test_batch in test_batches:\n",
    "                x_test_batch, y_test_batch = zip(*test_batch)\n",
    "                preds, test_loss = test_step(x_test_batch, y_test_batch)\n",
    "                sum_loss += test_loss\n",
    "                res = np.absolute(preds - np.argmax(y_test_batch, axis=1))\n",
    "                y_preds[index:index+len(res)] = res\n",
    "                i += 1\n",
    "                index += len(res)\n",
    "\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            acc = np.count_nonzero(y_preds==0)/len(y_preds)\n",
    "            acc_list.append(acc)\n",
    "            print(\"{}: Evaluation Summary, Loss {:g}, Acc {:g}\".format(time_str, sum_loss/i, acc))\n",
    "            print(\"{}: Current Max Acc {:g} with in Iteration {}\".format(time_str, max(acc_list), int(acc_list.index(max(acc_list))*evaluate_every)))\n",
    "\n",
    "            if max(acc_list) > vgg_known_acc_max[folder_to_test - 1]:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved current model checkpoint with max accuracy to {}\\n\".format(path))\n",
    "\n",
    "                \n",
    "if __name__ == \"__main__\":\n",
    "    main(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow-Py3",
   "language": "python",
   "name": "tensorflow-py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
